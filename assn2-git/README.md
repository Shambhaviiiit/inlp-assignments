<!-- ### Embedding_dim = 10; Hidden_dim = 128 (without much data preprocessing)
- Average Perplexity for 3-gram LM: 224249431451994.22
- Average Perplexity for 5-gram LM: 401489.63280091883

### Embedding_dim = 50; Hidden_dim = 256
- Average Perplexity for 3-gram LM: 133890959252.97466
- Average Perplexity for 5-gram LM: 89983578187867.2

### Embedding_dim = 128; Hidden_dim = 512
- Average Perplexity for 3-gram LM: 6.591122514969873e+22
- Average Perplexity for 5-gram LM: inf -->

## Feed Forward Neural Network - Pride and Prejudice
- Average Perplexity for 3-gram LM on training dataset: 47.39556121826172
- Average Perplexity for 5-gram LM on training dataset: 39.426231384277344

## Vanilla Recurrent Neural Network - Pride and Prejudice
-  Average Perplexity for 3-gram LM on training dataset: 59.308475494384766
-  Average Perplexity for 5-gram LM on training dataset: 59.16526794433594

## Long Short-Term Memory Language Model - Pride and Prejudice
- Average Perplexity for 3-gram LM on training dataset: 59.72645568847656
- Average Perplexity for 5-gram LM on training dataset: 56.50226974487305

## Vanilla Recurrent Neural Network - Ulysses
- Average Perplexity for 3-gram LM on training dataset: 87.85037231445312
- Average Perplexity for 5-gram LM on training dataset: 82.12122344970703

## Feed Forward Neural Network - Ulysses
- Average Perplexity for 3-gram LM on training dataset: 70.0067367553711
- Average Perplexity for 5-gram LM on training dataset: 64.32366180419922

## Long Short-Term Memory Language Model - Ulysses
- Average Perplexity for 3-gram LM on training dataset: 94.251220703125
- Average Perplexity for 5-gram LM on training dataset: 85.57880401611328